{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11195079,"sourceType":"datasetVersion","datasetId":6989174}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install relevant packages","metadata":{}},{"cell_type":"code","source":"!pip install bitsandbytes faiss-cpu sentence-transformers datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T14:25:50.248552Z","iopub.execute_input":"2025-04-08T14:25:50.248736Z","iopub.status.idle":"2025-04-08T14:26:00.618004Z","shell.execute_reply.started":"2025-04-08T14:25:50.248718Z","shell.execute_reply":"2025-04-08T14:26:00.616868Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Import all relevant libraries","metadata":{}},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\nimport datasets\nimport faiss\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport pandas as pd\nimport re","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T14:26:02.709750Z","iopub.execute_input":"2025-04-08T14:26:02.710108Z","iopub.status.idle":"2025-04-08T14:26:38.942230Z","shell.execute_reply.started":"2025-04-08T14:26:02.710076Z","shell.execute_reply":"2025-04-08T14:26:38.941487Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.environ[\"HUGGINGFACE_TOKEN\"] = \"your_token\"\nlogin(os.environ[\"HUGGINGFACE_TOKEN\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T14:26:40.721776Z","iopub.execute_input":"2025-04-08T14:26:40.722418Z","iopub.status.idle":"2025-04-08T14:26:40.903584Z","shell.execute_reply.started":"2025-04-08T14:26:40.722391Z","shell.execute_reply":"2025-04-08T14:26:40.902917Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = \"chengxuphd/liar2\"\ndataset = datasets.load_dataset(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T14:26:43.593617Z","iopub.execute_input":"2025-04-08T14:26:43.593956Z","iopub.status.idle":"2025-04-08T14:26:47.397382Z","shell.execute_reply.started":"2025-04-08T14:26:43.593930Z","shell.execute_reply":"2025-04-08T14:26:47.396438Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert labels: {0, 1, 2, 3} -> 1 (aka fake), {4, 5} -> 0 (aka real)\ndef convert_labels(data):\n    if data[\"label\"] in [0, 1, 2, 3]:\n        data[\"true_label\"] = 1\n    else:\n        data[\"true_label\"] = 0\n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T14:26:50.348674Z","iopub.execute_input":"2025-04-08T14:26:50.348960Z","iopub.status.idle":"2025-04-08T14:26:50.353147Z","shell.execute_reply.started":"2025-04-08T14:26:50.348937Z","shell.execute_reply":"2025-04-08T14:26:50.352316Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = dataset.map(convert_labels)\ntest_dataset = dataset[\"test\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T14:26:53.192242Z","iopub.execute_input":"2025-04-08T14:26:53.192609Z","iopub.status.idle":"2025-04-08T14:26:57.303004Z","shell.execute_reply.started":"2025-04-08T14:26:53.192582Z","shell.execute_reply":"2025-04-08T14:26:57.301880Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load embedding model\nembedding_model = SentenceTransformer(\"all-mpnet-base-v2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T14:26:58.669648Z","iopub.execute_input":"2025-04-08T14:26:58.669966Z","iopub.status.idle":"2025-04-08T14:27:04.403592Z","shell.execute_reply.started":"2025-04-08T14:26:58.669939Z","shell.execute_reply":"2025-04-08T14:27:04.402773Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Compute embeddings for justifications","metadata":{}},{"cell_type":"code","source":"# Extract justifications\njustifications = [sample[\"justification\"] for sample in test_dataset]\n\n# Convert justifications into embeddings\njustification_embeddings = embedding_model.encode(justifications, convert_to_numpy=True)\n\n# Create FAISS index\ndimension = justification_embeddings.shape[1]  # Get embedding size\nfaiss_index = faiss.IndexFlatL2(dimension)  # L2 (Euclidean) distance\nfaiss_index.add(justification_embeddings)   # Store embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T13:31:53.126912Z","iopub.execute_input":"2025-04-05T13:31:53.127226Z","iopub.status.idle":"2025-04-05T13:32:17.300139Z","shell.execute_reply.started":"2025-04-05T13:31:53.127192Z","shell.execute_reply":"2025-04-05T13:32:17.299297Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def retrieve_top_k_justifications(claim, k=2):\n    \"\"\"Retrieves top-k relevant justifications using FAISS\"\"\"\n    \n    # Convert claim into embedding\n    claim_embedding = embedding_model.encode([claim], convert_to_numpy=True)\n    \n    # Retrieve top-k justifications\n    distances, indices = faiss_index.search(claim_embedding, k)\n    \n    # Return retrieved justifications\n    return [justifications[i] for i in indices[0]], indices[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T14:13:03.237952Z","iopub.execute_input":"2025-04-05T14:13:03.238322Z","iopub.status.idle":"2025-04-05T14:13:03.243197Z","shell.execute_reply.started":"2025-04-05T14:13:03.238299Z","shell.execute_reply":"2025-04-05T14:13:03.241958Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def format_chat_prompt_v1(claim, speaker, speaker_description):\n    \"\"\"Formats the input prompt using retrieved justifications as context. (zero-shot)\"\"\"\n\n    # Retrieve relevant justifications\n    retrieved_justifications, indices = retrieve_top_k_justifications(claim, k=2)\n    justification_text = \"\\n\\n\".join(retrieved_justifications)  # Combine into one text block\n    indices = indices.tolist()\n    \n    chat_prompt = tokenizer.apply_chat_template(\n        [\n            {\"role\": \"system\", \"content\": \"You are an AI that detects fake news using both linguistic analysis and contextual information retrieved from a knowledge base.\"},\n\n            # Pass retrieved justifications as context\n            {\"role\": \"user\", \"content\": f\"\"\"Here is the relevant information from the knowledge base:\\n\\n\"{justification_text}\"\n\nClassify the following statement as FAKE or REAL, and highlight only the top five single keywords contributing to the classification.\n\nFocus on both linguistic features and the given contextual evidence.\n\nStatement: \"{claim}\" \n\nSpeaker: \"{speaker}\"\n\nSpeaker Description: \"{speaker_description}\"\n            \nOutput the result in the following structured format:\n    \nClassification: (FAKE or REAL)\n    \nKeywords: [list of ONLY five single keywords]\n    \nReasoning: (Explain based on both linguistic features and the given contextual evidence)\"\"\"}\n        ],\n        tokenize=False,\n        add_generation_prompt=True,\n    )\n\n    return chat_prompt, justification_text, indices","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T14:13:05.948384Z","iopub.execute_input":"2025-04-05T14:13:05.948737Z","iopub.status.idle":"2025-04-05T14:13:05.954073Z","shell.execute_reply.started":"2025-04-05T14:13:05.948699Z","shell.execute_reply":"2025-04-05T14:13:05.952979Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def format_chat_prompt_v2(claim, speaker, speaker_description):\n    \"\"\"Formats the input prompt using retrieved justifications as context. (two-shot)\"\"\"\n\n    # Retrieve relevant justifications\n    retrieved_justifications, indices = retrieve_top_k_justifications(claim, k=2)\n    justification_text = \"\\n\\n\".join(retrieved_justifications)  # Combine into one text block\n    indices = indices.tolist()\n    \n    chat_prompt = tokenizer.apply_chat_template(\n        [\n            {\"role\": \"system\", \"content\": \"You are an AI that detects fake news using both linguistic analysis and contextual information retrieved from a knowledge base.\"},\n\n            # Example 1 (FAKE)\n            {\"role\": \"user\", \"content\": f\"\"\"Here is the relevant information from the knowledge base:\n\n\"According to CBS News White House reporter Mark Knoller, who keeps records of presidential travel, President Obama took 29 vacation trips over his eight years in the White House. The trips spanned all or part of 235 days (not 692). He also made 39 visits to Camp David, totaling around 93 days. Even though the original story was published with a disclaimer, versions of the account were copied and pasted in social media posts with no such explanation and circulated as real news.  We rate this !\"\n\nClassify the following statement as FAKE or REAL, and highlight only the top five single keywords contributing to the classification.\n\nFocus on both linguistic features and the given contextual evidence.\n\nStatement: \"President Obama used government funds to pay for his personal vacations for the next 20 years and Trump sent him a bill.\" \n\nSpeaker: \"facebook posts\"\n\nSpeaker Description: \"Posters on Facebook and other social media networks.\"\n            \nOutput the result in the following structured format:\n    \nClassification: (FAKE or REAL)\n    \nKeywords: [list of ONLY five single keywords]\n    \nReasoning: (Explain based on both linguistic features and the given contextual evidence)\"\"\"},\n\n            {\"role\": \"assistant\", \"content\": \"\"\"Classification: FAKE\n            \nKeywords: Obama, vacations, government, bill, Trump\n            \nReasoning: Linguistically, the statement uses sensational and emotionally charged language (“used government funds,” “for the next 20 years,” “Trump sent him a bill”) common in fake news. The exaggerated future timeline and improbable scenario (a former president receiving a bill from a sitting or later president) lacks credibility and factual basis.\n\nFrom the contextual evidence, it is confirmed that President Obama took 29 vacation trips over eight years, totaling 235 days, and not 692 as claimed in viral posts. There is no verified report of misuse of government funds for future vacations or any bill sent by Trump. The original false claim was widely circulated on social media without context, further supporting that this is misinformation.\"\"\"},\n\n            # Example 2 (REAL)\n            {\"role\": \"user\", \"content\": f\"\"\"Here is the relevant information from the knowledge base:\n\n\"Here is the relevant information from the knowledge base:\nIn sum, Graves’ numbers are correct. The VA’s budget has increased by more than 40 percent, and the number of pending claims has skyrocketed from some 391,000 to 890,000 under the Obama administration. Graves also was accurate in noting that the increase took place under the president’s watch. While the VA’s benefits system has been troubled since well before Obama took office, efforts during his administration have not prepared the department for its current challenges. We rate Graves’ statement .\"\n\nClassify the following statement as FAKE or REAL, and highlight only the top five single keywords contributing to the classification.\n\nFocus on both linguistic features and the given contextual evidence.\n\nStatement: \"Despite having their budget increased by over 40 percent since 2009 … pending claims for benefits with the (Department of Veterans Affairs) have increased from 391,000 to 890,000 under the Obama Administration.\" \n\nSpeaker: \"tom graves\"\n\nSpeaker Description: \"Tom Graves represents Georgia's 9th Congressional District.\"\n            \nOutput the result in the following structured format:\n    \nClassification: (FAKE or REAL)\n    \nKeywords: [list of ONLY five single keywords]\n    \nReasoning: (Explain based on both linguistic features and the given contextual evidence)\"\"\"},\n\n            {\"role\": \"assistant\", \"content\": \"\"\"Classification: REAL\n            \nKeywords: budget, increased, claims, benefits, Obama\n            \nReasoning: Linguistically, the statement is factual in tone, presenting specific numerical data without exaggeration or emotionally charged language. The use of figures and administrative references (e.g., \"since 2009,\" \"Department of Veterans Affairs\") suggests a grounded, data-driven claim.\n\nFrom the contextual evidence, the knowledge base confirms that the VA’s budget increased by over 40 percent under the Obama administration and that pending claims rose from approximately 391,000 to 890,000 during the same period. It also notes that although the issues existed before Obama’s tenure, the surge in claims and budget increase indeed occurred “under the president’s watch.” Therefore, the statement made by Tom Graves is consistent with the verified information.\"\"\"},\n\n            # Pass retrieved justifications as context\n            {\"role\": \"user\", \"content\": f\"\"\"Here is the relevant information from the knowledge base:\\n\\n\"{justification_text}\"\n\nClassify the following statement as FAKE or REAL, and highlight only the top five single keywords contributing to the classification.\n\nFocus on both linguistic features and the given contextual evidence.\n\nStatement: \"{claim}\" \n\nSpeaker: \"{speaker}\"\n\nSpeaker Description: \"{speaker_description}\"\n            \nOutput the result in the following structured format:\n    \nClassification: (FAKE or REAL)\n    \nKeywords: [list of ONLY five single keywords]\n    \nReasoning: (Explain based on both linguistic features and the given contextual evidence)\"\"\"}\n        ],\n        tokenize=False,\n        add_generation_prompt=True,\n    )\n\n    return chat_prompt, justification_text, indices","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T14:13:08.915727Z","iopub.execute_input":"2025-04-05T14:13:08.916013Z","iopub.status.idle":"2025-04-05T14:13:08.922833Z","shell.execute_reply.started":"2025-04-05T14:13:08.915991Z","shell.execute_reply":"2025-04-05T14:13:08.921917Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load model\nquantization_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.float16,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", quantization_config=quantization_config, device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-05T14:13:13.633598Z","iopub.execute_input":"2025-04-05T14:13:13.633887Z","iopub.status.idle":"2025-04-05T14:15:08.425417Z","shell.execute_reply.started":"2025-04-05T14:13:13.633864Z","shell.execute_reply":"2025-04-05T14:15:08.424762Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract classification, keywords, and reasoning from model output\ndef extract_prediction(response):\n    # Extract classification (FAKE or REAL)\n    classification_match = re.search(r'Classification:\\s*(FAKE|REAL)', response)\n    classification = classification_match.group(1) if classification_match else \"UNKNOWN\"\n\n    # Extract keywords as a list\n    keywords_match = re.search(r'Keywords:\\s*(.*)', response)\n    \n    if keywords_match:\n        keywords_text = keywords_match.group(1).strip()\n        \n        # Split by commas and remove any stray quotes/spaces\n        keywords = [kw.strip().strip('\"').strip(\"'\") for kw in keywords_text.split(\", \")]\n    else:\n        keywords = []\n\n    # Extract reasoning\n    reasoning_match = re.search(r'Reasoning:\\s*(.*)', response, re.DOTALL)\n    reasoning = reasoning_match.group(1).strip() if reasoning_match else \"No reasoning provided.\"\n    \n    return classification, keywords, reasoning","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T14:16:41.338562Z","iopub.execute_input":"2025-04-05T14:16:41.338905Z","iopub.status.idle":"2025-04-05T14:16:41.344183Z","shell.execute_reply.started":"2025-04-05T14:16:41.338879Z","shell.execute_reply":"2025-04-05T14:16:41.343277Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Perform evaluation to extract results from test dataset into a .csv file\ndef evaluate(shot=0):\n    results = []\n    unknown_count = 0\n    \n    for sample in test_dataset:\n        unique_id = sample[\"id\"]\n        claim = sample[\"statement\"]\n        speaker = sample[\"speaker\"]\n        speaker_description = sample[\"speaker_description\"]\n        true_label = sample[\"true_label\"]\n        orig_label = sample[\"label\"]\n    \n        # Format prompt using chat template\n        if shot == 0: # zero-shot\n            formatted_prompt, justification_text, indices = format_chat_prompt_v1(claim, speaker, speaker_description)\n        else: # few-shot\n            formatted_prompt, justification_text, indices = format_chat_prompt_v2(claim, speaker, speaker_description)\n    \n        # Tokenize and generate response\n        inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n        output_tokens = model.generate(**inputs, max_new_tokens=250)\n        model_response = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n    \n        cleaned_response = model_response.split(\"[/INST]\")[-1].strip()\n    \n        # Extract structured outputs\n        predicted_label, keywords, reasoning = extract_prediction(cleaned_response)\n    \n        # Convert predicted label to binary (FAKE=1, REAL=0)\n        if predicted_label == \"FAKE\":\n            pred_label = 1\n        elif predicted_label == \"REAL\":\n            pred_label = 0\n        else: # Unknown label predicted\n            print(predicted_label)\n            pred_label = -1\n            unknown_count += 1\n        \n        results.append({\n                \"id\": unique_id,\n                \"Claim\": claim,\n                \"Retrieved Justification\": justification_text,\n                \"Retrieved Index\": indices,\n                \"Prediction\": pred_label,\n                \"Keywords\": \", \".join(keywords),\n                \"Reasoning\": reasoning,\n                \"True Label\": true_label,\n                \"Original Label\": orig_label\n            })\n    \n    df = pd.DataFrame(results)\n    df.to_csv(\"results_zeroshot_rag.csv\", index=False)\n    print(f\"Number of test instances where model outputs an unknown classification: {unknown_count}\")\n    print(f\"% of test instances where model outputs either FAKE or REAL: {(len(test_dataset) - unknown_count) / len(test_dataset)*100:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:29:01.105902Z","iopub.execute_input":"2025-03-29T05:29:01.106231Z","iopub.status.idle":"2025-03-29T05:29:01.114531Z","shell.execute_reply.started":"2025-03-29T05:29:01.106202Z","shell.execute_reply":"2025-03-29T05:29:01.113689Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluate(shot=0) # zero-shot","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluation Metrics\n\ndf = pd.read_csv('results_zeroshot_rag.csv')\n\n# Filter out rows with Prediction == -1\ndf = df[df[\"Prediction\"] != -1]\n\ndf_class_0 = df[df[\"True Label\"] == 0]\ndf_class_1 = df[df[\"True Label\"] == 1]\n\nminority_class_size = min(len(df_class_0), len(df_class_1))\n\nmetrics = {\n    'accuracy': [],\n    'precision': [],\n    'recall': [],\n    'f1': []\n}\n\n# Run the sampling and evaluation 5 times\nfor _ in range(5):\n    if len(df_class_0) < len(df_class_1):\n        sampled_majority = df_class_1.sample(n=minority_class_size, random_state=np.random.randint(0, 10000))\n        balanced_df = pd.concat([df_class_0, sampled_majority])\n    else:\n        sampled_majority = df_class_0.sample(n=minority_class_size, random_state=np.random.randint(0, 10000))\n        balanced_df = pd.concat([df_class_1, sampled_majority])\n\n    # Shuffle the balanced dataset\n    balanced_df = balanced_df.sample(frac=1).reset_index(drop=True)\n\n    y_true = balanced_df[\"True Label\"]\n    y_pred = balanced_df[\"Prediction\"]\n\n    # Compute metrics\n    metrics['accuracy'].append(accuracy_score(y_true, y_pred))\n    metrics['precision'].append(precision_score(y_true, y_pred))\n    metrics['recall'].append(recall_score(y_true, y_pred))\n    metrics['f1'].append(f1_score(y_true, y_pred))\n\n# Compute and print average metrics\navg_metrics = {k: np.mean(v) for k, v in metrics.items()}\nprint(\"Averaged Metrics over 5 runs:\")\nfor metric, score in avg_metrics.items():\n    print(f\"{metric.capitalize()}: {score:.2f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Debugging Purposes","metadata":{}},{"cell_type":"code","source":"# for i in range(10):\n#     test_claim = test_dataset[\"statement\"][i]\n#     test_speaker = test_dataset[\"speaker\"][i]\n#     test_desc = test_dataset[\"speaker_description\"][i]\n#     test_label = test_dataset[\"true_label\"][i]\n#     # print(f\"Claim:\\n{train_claim}\")\n#     formatted_prompt, _, _ = format_chat_prompt_v2(test_claim, test_speaker, test_desc)\n#     # print(f\"Prompt:\\n{formatted_prompt}\")\n    \n#     inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n#     output_tokens = model.generate(**inputs, max_new_tokens=250)\n#     model_response = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n    \n#     cleaned_response = model_response.split(\"[/INST]\")[-1].strip()\n#     print(f\"Model Response:\\n{cleaned_response}\")\n#     print(f\"True Label:\\n{test_label}\")\n\n#     print(extract_prediction(cleaned_response))\n#     print(\"\\n#########################\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T14:26:55.658169Z","iopub.execute_input":"2025-04-05T14:26:55.658547Z","iopub.status.idle":"2025-04-05T14:29:00.871306Z","shell.execute_reply.started":"2025-04-05T14:26:55.658488Z","shell.execute_reply":"2025-04-05T14:29:00.870588Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for i in range(10):\n#     train_claim = dataset[\"train\"][\"statement\"][i]\n#     train_label = dataset[\"train\"][\"label\"][i]\n#     print(f\"Claim:\\n{train_claim}\")\n#     formatted_prompt = format_chat_prompt_v2(train_claim)\n    \n#     inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n#     output_tokens = model.generate(**inputs, max_new_tokens=150)\n#     model_response = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n    \n#     cleaned_response = model_response.split(\"[/INST]\")[-1].strip()\n#     print(f\"Model Response:\\n{cleaned_response}\")\n#     print(f\"True Label:\\n{train_label}\")\n    \n#     print(extract_prediction(cleaned_response))\n#     print(\"\\n#########################\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T06:15:25.338273Z","iopub.execute_input":"2025-03-29T06:15:25.338575Z","iopub.status.idle":"2025-03-29T06:16:28.992283Z","shell.execute_reply.started":"2025-03-29T06:15:25.338551Z","shell.execute_reply":"2025-03-29T06:16:28.991461Z"}},"outputs":[],"execution_count":null}]}