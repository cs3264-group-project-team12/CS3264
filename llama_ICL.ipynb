{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T14:21:08.537897Z",
     "iopub.status.busy": "2025-04-05T14:21:08.537594Z",
     "iopub.status.idle": "2025-04-05T14:21:17.758389Z",
     "shell.execute_reply": "2025-04-05T14:21:17.757580Z",
     "shell.execute_reply.started": "2025-04-05T14:21:08.537866Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Downloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl (76.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.45.4\n"
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T14:21:17.759599Z",
     "iopub.status.busy": "2025-04-05T14:21:17.759382Z",
     "iopub.status.idle": "2025-04-05T14:21:17.763495Z",
     "shell.execute_reply": "2025-04-05T14:21:17.762570Z",
     "shell.execute_reply.started": "2025-04-05T14:21:17.759579Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T14:21:17.765532Z",
     "iopub.status.busy": "2025-04-05T14:21:17.765342Z",
     "iopub.status.idle": "2025-04-05T14:21:17.784243Z",
     "shell.execute_reply": "2025-04-05T14:21:17.783653Z",
     "shell.execute_reply.started": "2025-04-05T14:21:17.765516Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T14:21:17.785676Z",
     "iopub.status.busy": "2025-04-05T14:21:17.785457Z",
     "iopub.status.idle": "2025-04-05T14:21:18.700436Z",
     "shell.execute_reply": "2025-04-05T14:21:18.699832Z",
     "shell.execute_reply.started": "2025-04-05T14:21:17.785657Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(os.environ[\"HUGGINGFACE_TOKEN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T14:21:18.701380Z",
     "iopub.status.busy": "2025-04-05T14:21:18.701152Z",
     "iopub.status.idle": "2025-04-05T14:21:23.003165Z",
     "shell.execute_reply": "2025-04-05T14:21:23.002485Z",
     "shell.execute_reply.started": "2025-04-05T14:21:18.701360Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d0a8229c104221bdf63ab96b78c31a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/2.87k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d288d05042cb4d9ea9687630325cad1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.csv:   0%|          | 0.00/19.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e6761a1b4c8464e92209ab4f489c2e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid.csv:   0%|          | 0.00/2.38M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc89dedc97c34b57824238dab6529823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.csv:   0%|          | 0.00/2.38M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f2f2c07cf934faeac45fe833b498ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/18369 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7b22186b0ba4214a3f43fc6adf56ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/2297 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82282583306348828cdd4c1fecded112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2296 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = \"chengxuphd/liar2\"\n",
    "dataset = datasets.load_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T14:21:23.004178Z",
     "iopub.status.busy": "2025-04-05T14:21:23.003848Z",
     "iopub.status.idle": "2025-04-05T14:21:23.007836Z",
     "shell.execute_reply": "2025-04-05T14:21:23.007189Z",
     "shell.execute_reply.started": "2025-04-05T14:21:23.004158Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert labels: {0, 1, 2} -> 0, {3, 4, 5} -> 1\n",
    "def convert_labels(data):\n",
    "    if data[\"label\"] in [0, 1, 2]:\n",
    "        data[\"label\"] = 0\n",
    "    else:\n",
    "        data[\"label\"] = 1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T14:21:23.009165Z",
     "iopub.status.busy": "2025-04-05T14:21:23.008829Z",
     "iopub.status.idle": "2025-04-05T14:21:26.511157Z",
     "shell.execute_reply": "2025-04-05T14:21:26.510149Z",
     "shell.execute_reply.started": "2025-04-05T14:21:23.009105Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2949ea3be44c41ffbc60f32e58baeb6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18369 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b49db2708e204b379a1fa7ba2263a721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2297 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "335df6e74faf494982cbc99ff3109850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2296 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(convert_labels)\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T14:21:26.512356Z",
     "iopub.status.busy": "2025-04-05T14:21:26.512044Z",
     "iopub.status.idle": "2025-04-05T14:21:26.515492Z",
     "shell.execute_reply": "2025-04-05T14:21:26.514900Z",
     "shell.execute_reply.started": "2025-04-05T14:21:26.512328Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"meta-llama/few-shot\"\n",
    "CHECKPOINT_PATH = f'{MODEL_NAME}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T16:02:00.868273Z",
     "iopub.status.busy": "2025-04-04T16:02:00.867992Z",
     "iopub.status.idle": "2025-04-04T16:02:00.887905Z",
     "shell.execute_reply": "2025-04-04T16:02:00.886981Z",
     "shell.execute_reply.started": "2025-04-04T16:02:00.868248Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'meta-llama/zero-shot' already exists.\n"
     ]
    }
   ],
   "source": [
    "def create_directory_if_not_exists(dir_path):\n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(dir_path):\n",
    "        # Create the directory (and any necessary parent directories)\n",
    "        os.makedirs(dir_path)\n",
    "        print(f\"Directory '{dir_path}' created.\")\n",
    "    else:\n",
    "        print(f\"Directory '{dir_path}' already exists.\")\n",
    "        \n",
    "create_directory_if_not_exists(CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-05T14:21:26.517671Z",
     "iopub.status.busy": "2025-04-05T14:21:26.517438Z",
     "iopub.status.idle": "2025-04-05T14:23:07.654173Z",
     "shell.execute_reply": "2025-04-05T14:23:07.653168Z",
     "shell.execute_reply.started": "2025-04-05T14:21:26.517647Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87be66302a2d466995134466524f03be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde74280153a42ff84563ddf44c77f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1567c518dc6e419bb2fd3959b5220881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa088b0cdb04fb3940ed14c5107468e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded4a2e4475d4586b2842b69550b9105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2629c76b455f43a1953c0253e3a2a032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c64b2a7f58104990b9c9768557b0273a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397d73fe6dc54b9f87a91c99f474660b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab85dede25447b6a0d8440a445d7493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aedd820bc0ab42c18bb939b8b1e68ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f61b87541941538db90a5aff8a90ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9768fc04aab54f728951248647466b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Use instruction-tuned models\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T14:23:07.659042Z",
     "iopub.status.busy": "2025-04-05T14:23:07.658753Z",
     "iopub.status.idle": "2025-04-05T14:23:07.667124Z",
     "shell.execute_reply": "2025-04-05T14:23:07.666466Z",
     "shell.execute_reply.started": "2025-04-05T14:23:07.659012Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Zero-shot prompt template\n",
    "def format_chat_prompt_v1(claim):\n",
    "    \"\"\"Formats input using the chat template for better responses (zero-shot).\"\"\"\n",
    "    user_input = f\"\"\"Classify the following claim as FAKE or REAL, and highlight only the top five keywords contributing to the classification.\n",
    "    \n",
    "    Focus only on linguistic features (tone, exaggeration, emotional appeal) and do NOT use any factual knowledge.\n",
    "    \n",
    "    Claim: \"{claim}\"\n",
    "    \n",
    "    Output the result in the following structured format:\n",
    "    \n",
    "    Classification: (FAKE or REAL)\n",
    "    \n",
    "    Keywords: list of ONLY five important words without explanation, in the format: [kw1, kw2, kw3, kw4, kw5]\n",
    "    \n",
    "    Linguistic Reasoning: (Explain based on language features such as exaggeration, strong emotions, vagueness, etc.)\"\"\"\n",
    "\n",
    "    chat_prompt = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI that detects fake news using linguistic analysis, focusing on tone and word choice, without relying on factual knowledge.\"},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ],\n",
    "        tokenize=False,  # Keep raw string for LLM\n",
    "        add_generation_prompt=True,  # Adds assistant's turn automatically\n",
    "    )\n",
    "    return chat_prompt\n",
    "\n",
    "# Few-shot prompt template\n",
    "def format_chat_prompt_v2(claim):\n",
    "    \"\"\"Formats input using the chat template for better responses with few-shot examples.\"\"\"\n",
    "\n",
    "    chat_prompt = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI that classifies a piece of text as FAKE or REAL, using linguistic analysis, focusing on tone and word choice, without relying on factual knowledge.\"},\n",
    "            \n",
    "            # Example 1 (FAKE)\n",
    "            {\"role\": \"user\", \"content\": \"\"\"Classify the following claim as FAKE or REAL, and highlight only the top five keywords contributing to the classification.\n",
    "\n",
    "            Do NOT give any classification other than FAKE or REAL.\n",
    "            \n",
    "            Focus only on linguistic features (tone, exaggeration, emotional appeal) and do NOT use any factual knowledge.\n",
    "            \n",
    "            Claim: \"President Obama used government funds to pay for his personal vacations for the next 20 years and Trump sent him a bill.\"\n",
    "            \n",
    "            Output the result in the following structured format:\n",
    "    \n",
    "            Classification: (FAKE or REAL)\n",
    "    \n",
    "            Keywords: list of ONLY five important words without explanation, in the format: [kw1, kw2, kw3, kw4, kw5]\n",
    "    \n",
    "            Linguistic Reasoning: (Explain based on language features such as exaggeration, strong emotions, vagueness, etc.)\"\"\"},\n",
    "            \n",
    "            {\"role\": \"assistant\", \"content\": \"\"\"Classification: FAKE\n",
    "            \n",
    "            Keywords: [\"government funds\", \"personal vacations\", \"20 years\", \"Trump\", \"bill\"]\n",
    "            \n",
    "            Linguistic Reasoning: The claim uses exaggeration, sensational language, and vague accusations (e.g., \"20 years\", \"government funds\") to create drama and suspicion without evidence.\"\"\"},\n",
    "\n",
    "            # Example 2 (REAL)\n",
    "            {\"role\": \"user\", \"content\": \"\"\"Classify the following claim as FAKE or REAL, and highlight only the top five keywords contributing to the classification.\n",
    "            \n",
    "            Do NOT give any classification other than FAKE or REAL.\n",
    "            \n",
    "            Focus only on linguistic features (tone, exaggeration, emotional appeal) and do NOT use any factual knowledge.\n",
    "            \n",
    "            Claim: \"Despite having their budget increased by over 40 percent since 2009, pending claims for benefits with the Department of Veterans Affairs have increased from 391,000 to 890,000 under the Obama Administration.\"\n",
    "            \n",
    "            Output the result in the following structured format:\n",
    "    \n",
    "            Classification: (FAKE or REAL)\n",
    "    \n",
    "            Keywords: list of ONLY five important words without explanation, in the format: [kw1, kw2, kw3, kw4, kw5]\n",
    "    \n",
    "            Linguistic Reasoning: (Explain based on language features such as exaggeration, strong emotions, vagueness, etc.)\"\"\"},\n",
    "            \n",
    "            {\"role\": \"assistant\", \"content\": \"\"\"Classification: REAL\n",
    "            \n",
    "            Keywords: [\"budget\", \"40 percent\", \"pending claims\", \"Department of Veterans Affairs\", \"Obama Administration\"]\n",
    "            \n",
    "            Linguistic Reasoning: The claim presents specific figures, neutral wording, and clear attribution, making it sound objective and fact-based rather than emotional or exaggerated.\"\"\"},\n",
    "\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"Classify the following claim as FAKE or REAL, and highlight only the top five keywords contributing to the classification.\n",
    "            \n",
    "            Do NOT give any classification other than FAKE or REAL.\n",
    "            \n",
    "            Focus only on linguistic features (tone, exaggeration, emotional appeal) and do NOT use any factual knowledge.\n",
    "            \n",
    "            Claim: \"{claim}\"\n",
    "            \n",
    "            Output the result in the following structured format:\n",
    "    \n",
    "            Classification: (FAKE or REAL)\n",
    "    \n",
    "            Keywords: list of ONLY five important words without explanation, in the format: [kw1, kw2, kw3, kw4, kw5]\n",
    "    \n",
    "            Linguistic Reasoning: (Explain based on language features such as exaggeration, strong emotions, vagueness, etc.)\"\"\"}\n",
    "        ],\n",
    "        tokenize=False,  # Keep raw string for LLM\n",
    "        add_generation_prompt=True,  # Adds assistant's turn automatically\n",
    "    )\n",
    "    \n",
    "    return chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T14:23:07.669352Z",
     "iopub.status.busy": "2025-04-05T14:23:07.669047Z",
     "iopub.status.idle": "2025-04-05T14:23:10.749705Z",
     "shell.execute_reply": "2025-04-05T14:23:10.748975Z",
     "shell.execute_reply.started": "2025-04-05T14:23:07.669322Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_prediction(response):\n",
    "    \"\"\"Extracts classification, keywords, and reasoning from model output.\"\"\"\n",
    "    \n",
    "    # Extract classification (FAKE or REAL)\n",
    "    classification_match = re.search(r'Classification:\\s*(FAKE|REAL)', response)\n",
    "    classification = classification_match.group(1) if classification_match else \"UNKNOWN\"\n",
    "\n",
    "    # print(response)\n",
    "\n",
    "    # Extract keywords as a list\n",
    "    keywords_match = re.search(r'Keywords:\\s*\\n([\\s\\S]*)', response)\n",
    "\n",
    "    if keywords_match:\n",
    "        keywords_text = keywords_match.group(1).strip()\n",
    "        \n",
    "        # Extract keywords handling quoted and unquoted words, and ignoring explanations after `-`\n",
    "        keywords = re.findall(r'\\d+\\.\\s*[\"“”]?(.*?)[”\"]?(?:\\s*-\\s*.*)?$', keywords_text, re.MULTILINE)\n",
    "        keywords = [kw.strip() for kw in keywords]\n",
    "    else:\n",
    "        keywords = []\n",
    "\n",
    "    # Extract linguistic reasoning\n",
    "    reasoning_match = re.search(r'Linguistic Reasoning:\\s*(.*)', response, re.DOTALL)\n",
    "    reasoning = reasoning_match.group(1).strip() if reasoning_match else \"No reasoning provided.\"\n",
    "    \n",
    "    return classification, keywords, reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T14:23:10.751172Z",
     "iopub.status.busy": "2025-04-05T14:23:10.750828Z",
     "iopub.status.idle": "2025-04-05T14:23:10.772665Z",
     "shell.execute_reply": "2025-04-05T14:23:10.772048Z",
     "shell.execute_reply.started": "2025-04-05T14:23:10.751140Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "results = []\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T14:23:10.773858Z",
     "iopub.status.busy": "2025-04-05T14:23:10.773561Z",
     "iopub.status.idle": "2025-04-05T14:23:10.795066Z",
     "shell.execute_reply": "2025-04-05T14:23:10.794336Z",
     "shell.execute_reply.started": "2025-04-05T14:23:10.773830Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1899\n"
     ]
    }
   ],
   "source": [
    "with open(f'{CHECKPOINT_PATH}/true_labels.pkl', 'rb') as file:\n",
    "    true_labels = pickle.load(file)\n",
    "\n",
    "with open(f'{CHECKPOINT_PATH}/pred_labels.pkl', 'rb') as file:\n",
    "    pred_labels = pickle.load(file)\n",
    "\n",
    "with open(f'{CHECKPOINT_PATH}/results.pkl', 'rb') as file:\n",
    "    results = pickle.load(file)\n",
    "\n",
    "i = len(true_labels)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T14:23:10.795927Z",
     "iopub.status.busy": "2025-04-05T14:23:10.795737Z",
     "iopub.status.idle": "2025-04-05T15:16:35.689787Z",
     "shell.execute_reply": "2025-04-05T15:16:35.688803Z",
     "shell.execute_reply.started": "2025-04-05T14:23:10.795910Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1900 done\n",
      "2000 done\n",
      "2100 done\n",
      "2200 done\n"
     ]
    }
   ],
   "source": [
    "for sample in test_dataset.select(range(i, len(test_dataset))):\n",
    "    if i % 100 == 0:\n",
    "        print(f'{i} done')\n",
    "        with open(f'{CHECKPOINT_PATH}/true_labels.pkl', 'wb') as file:\n",
    "            pickle.dump(true_labels, file)\n",
    "        \n",
    "        with open(f'{CHECKPOINT_PATH}/pred_labels.pkl', 'wb') as file:\n",
    "            pickle.dump(pred_labels, file)\n",
    "\n",
    "        with open(f'{CHECKPOINT_PATH}/results.pkl', 'wb') as file:\n",
    "            pickle.dump(results, file)\n",
    "\n",
    "                \n",
    "    i += 1\n",
    "    \n",
    "    claim = sample[\"statement\"]\n",
    "    true_label = sample[\"label\"]\n",
    "\n",
    "    # Format prompt using chat template\n",
    "    formatted_prompt = format_chat_prompt_v2(claim)\n",
    "\n",
    "    # Tokenize and generate response\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    output_tokens = model.generate(**inputs, max_new_tokens=150)\n",
    "    model_response = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "    cleaned_response = model_response.split(\"[/INST]\")[-1].strip()\n",
    "\n",
    "    # Extract structured outputs\n",
    "    predicted_label, keywords, reasoning = extract_prediction(cleaned_response)\n",
    "    # print(keywords)\n",
    "\n",
    "    # Convert predicted label to binary (FAKE=0, REAL=1)\n",
    "    if predicted_label  == \"UNKNOWN\":\n",
    "        continue\n",
    "    pred_label = 0 if predicted_label == \"FAKE\" else 1\n",
    "\n",
    "    # Store results\n",
    "    true_labels.append(true_label)\n",
    "    pred_labels.append(pred_label)\n",
    "\n",
    "    # # Print one sample response for debugging\n",
    "    # print(f\"Claim: {claim}\")\n",
    "    # print(f\"Prediction: {predicted_label}\")\n",
    "    # print(f\"Keywords: {keywords}\")\n",
    "    # print(f\"Linguistic Reasoning: {reasoning}\\n\")\n",
    "    \n",
    "    results.append({\n",
    "            \"Claim\": claim,\n",
    "            \"Prediction\": predicted_label,\n",
    "            \"Keywords\": \", \".join(keywords),\n",
    "            \"Linguistic Reasoning\": reasoning,\n",
    "            \"True Label\": true_label\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(f\"{CHECKPOINT_PATH}/results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T15:16:35.691653Z",
     "iopub.status.busy": "2025-04-05T15:16:35.691317Z",
     "iopub.status.idle": "2025-04-05T15:16:35.698034Z",
     "shell.execute_reply": "2025-04-05T15:16:35.697310Z",
     "shell.execute_reply.started": "2025-04-05T15:16:35.691606Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'{CHECKPOINT_PATH}/true_labels.pkl', 'wb') as file:\n",
    "    pickle.dump(true_labels, file)\n",
    "\n",
    "with open(f'{CHECKPOINT_PATH}/pred_labels.pkl', 'wb') as file:\n",
    "    pickle.dump(pred_labels, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T15:16:35.699328Z",
     "iopub.status.busy": "2025-04-05T15:16:35.699071Z",
     "iopub.status.idle": "2025-04-05T15:16:35.777353Z",
     "shell.execute_reply": "2025-04-05T15:16:35.776464Z",
     "shell.execute_reply.started": "2025-04-05T15:16:35.699305Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6328\n",
      "Precision: 0.6868\n",
      "Recall: 0.2456\n",
      "F1-score: 0.3618\n"
     ]
    }
   ],
   "source": [
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(true_labels, pred_labels)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average=\"binary\")\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Llama**\n",
    "- Zero shot:\n",
    "```\n",
    "Accuracy: 0.4673\n",
    "Precision: 0.4420\n",
    "Recall: 0.9794\n",
    "F1-score: 0.6091\n",
    "```\n",
    "\n",
    "- Few shot:\n",
    "```\n",
    "Accuracy: 0.6328\n",
    "Precision: 0.6868\n",
    "Recall: 0.2456\n",
    "F1-score: 0.3618\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-04T16:02:17.504464Z",
     "iopub.status.idle": "2025-04-04T16:02:17.504835Z",
     "shell.execute_reply": "2025-04-04T16:02:17.504669Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     train_claim = dataset[\"train\"][\"statement\"][i]\n",
    "#     train_label = dataset[\"train\"][\"label\"][i]\n",
    "#     print(f\"Claim:\\n{train_claim}\")\n",
    "#     formatted_prompt = format_chat_prompt(train_claim)\n",
    "    \n",
    "#     inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "#     output_tokens = model.generate(**inputs, max_new_tokens=150)\n",
    "#     model_response = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "    \n",
    "#     cleaned_response = model_response.split(\"[/INST]\")[-1].strip()\n",
    "#     print(f\"Model Response:\\n{cleaned_response}\")\n",
    "#     print(f\"True Label:\\n{train_label}\")\n",
    "\n",
    "#     print(extract_prediction(cleaned_response))\n",
    "#     print(\"\\n#########################\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
